########################################################################################################################
Analysis. Project "Show Me the Data Structures"
########################################################################################################################

------------------------------------------------------------------------------------------------------------------------
#Problem1

In Python there is an OrderedDict which is a dict subclass that remembers the order entries were added.
The order in LRU starts from the least used element to recently used.
By using move_to_end() when get() and set() are called, I'm moving the key of the element to the end
and this makes the element 'recently used'. When the LRU size is full I'm using popitem(last=False) which
removes the 1st element - the least recently used.
All operations have O(1) time complexity in OrderedDict.


------------------------------------------------------------------------------------------------------------------------
#Problem2

find_files() uses os.listdir :
    Here we go through all items in the  dir_list - O(N).
    And recursively call find_files() which takes O(N).
    Operation append to the list takes O(1).
    The total time complexity is O(N*N)

We can also use glob.glob with recursive=True and however using the “**” pattern in large directory trees
may consume an inordinate amount of time.
Using of rglob() is like calling Path.glob() with “**/” added in front of the given relative pattern.
pathlib.Path is one of the bottlenecks that’s slowing the loop down-


------------------------------------------------------------------------------------------------------------------------
#Problem3

Analysing of the call of huffman_encoding():
     We create frequency dictionary which takes O(N) time where N is the amount of characters in the message.
     We build a tree from the frequency dictionary which takes O(N) times where N is the number of items in dict.
        to create heap_list using heappush takes O(logN)
        to merge the nodes using heappop and heappush also takes O(logN)
        we omit the coefficients so the result is O(logN). Since O(N) > O(logN) the total is O(N)
     To assign the binary codes takes O(N) where we first use heappop with O(logN) and
     than recursively do the same for each node O(N). Since O(N) > O(logN) the total is O(N).
     To get encoded data takes O(N) because we need to map each char to its code in the binary codes dict.
Total is O(N)
Analysing of the call of huffman_decoding():
     Takes O(N) time where N is the number of characters(bits) in the encoded message. Operations where we get the
     item from dictionary and getting the value takes O(1) time.
Total time complexity for main() function is O(N)


------------------------------------------------------------------------------------------------------------------------
#Problem4

We need to go through all elements in the forest. It has a depth N.
Each level has different amount of children - M groups and A users.
First we check user_name in the list of all users - takes O(A)
Than we check group in the all groups list - O(M) and recursively call is_user_in_group.
Each level will have as many calls as the number of groups - O(M^N).
O(A)+O(M)*O(M^N) = O(M^N+1)


------------------------------------------------------------------------------------------------------------------------
#Problem5

Usually when we write a code it suppose to ran as fast as possible. Blockchain implementation is an exception because
the purpose is to make addition of the block to the chain very hard by increasing the time of performing this operation.
For this we need to write a method proof_of_work() which recalculates the hash for the block in a long loop based on
the difficulty and the nonce. The difficulty is the number of leading zero bits in a hash. The nonce is a value which we
use to slightly modify a result of computed hash.
The proof_of_work stops recalculation of the hash when the resulted hash begins with the number of zeros provided in the
difficulty.
We also need to make sure that the the chain supports only append operation (no update allowed) and keeps the order.
Once we get a valid proof that satisfies the difficulty criteria we can add such block to the chain.
The process is commonly known as mining.
Time complexity of mining one block takes O(N) where N - the length of the hash string


------------------------------------------------------------------------------------------------------------------------
#Problem6

To perform union() of 2 linked lists will take O(N) where N is the number of items in a longest linked list.
We are iterating through the nodes of 2 lists simultaneously and add nodes to the set. After that we convert the
set to a linked list which also takes O(N). O(N)+O(N) = O(2N)
Since we can omit any coefficients the total time complexity is O(N).

For intersection I implemented 2 versions - intersection() and intersection_slower()
intersection_slower() takes longer time to perform because while iterating through nodes of one linked list we
perform a search operation of the node value in the second linked list. The search operation takes O(N) and doing
it in a 'while' for all nodes in first linked list will multiply the total result of complexity - O(N^2)
In intersection() I improved the complexity by converting the linked list to a set which takes O(N), then
iterating by nodes in second linked list and adding items to a set takes O(N), converting the resulted set to a linked
list takes O(N). O(N+N+N) = O(3N). So the total complexity is O(N).
